{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import os.path as path\n",
    "from torch.utils.data import Dataset\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "from skimage import io, transform\n",
    "from torchvision import transforms as torch_transform\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "DRESS_FOLDER = \"../data/json/\"\n",
    "\n",
    "ID_NAME = \"retrieval_dresses.json\"\n",
    "TEST_NAME = \"test_pairs_dresses.json\"\n",
    "TRAIN_NAME = \"train_pairs_dresses.json\"\n",
    "PHOTO_FILE = \"../data/photos/photos_dress.txt\"\n",
    "SEED = 17\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "from PIL import ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Step 1 upload dataset metadata and photos info</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'street2shop'...\n",
      "remote: Counting objects: 9, done.\u001b[K\n",
      "remote: Compressing objects: 100% (7/7), done.\u001b[K\n",
      "Unpacking objects: 100% (9/9), done.\n",
      "remote: Total 9 (delta 1), reused 9 (delta 1), pack-reused 0\u001b[K\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "  0     0    0     0    0     0      0      0 --:--:--  0:00:02 --:--:--     0^C\n",
      "mv: rename street2shop/meta to data/meta: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/pumpikano/street2shop\n",
    "!mkdir street2shop/images\n",
    "!bash street2shop/get_street2shop.sh\n",
    "!mkdir data\n",
    "!mv street2shop/meta data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Â <h1>step 2 extract street photos only</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def read_json(file_name):\n",
    "    with open(file_name) as f:\n",
    "        obj = json.loads(f.readline())\n",
    "    return obj\n",
    "\n",
    "\n",
    "def get_photo_ids(obj, retr_dict):\n",
    "    photos = set()\n",
    "    for item in obj:\n",
    "        photos.add(item[\"photo\"])\n",
    "        product_photos = retr_dict[item[\"product\"]]\n",
    "        photos.update(product_photos)\n",
    "    return photos\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ != \"__main__\":\n",
    "    id_to_photo_dirty = read_json(DRESS_FOLDER+ID_NAME)\n",
    "    id_to_photo_clean = {}\n",
    "    for item in id_to_photo_dirty:\n",
    "        if item[\"product\"] in id_to_photo_clean:\n",
    "            id_to_photo_clean[item[\"product\"]].append(item[\"photo\"])\n",
    "        else:\n",
    "            id_to_photo_clean[item[\"product\"]] = [item[\"photo\"]]\n",
    "    test_dress = read_json(DRESS_FOLDER + TEST_NAME)\n",
    "    train_dress = read_json(DRESS_FOLDER + TRAIN_NAME)\n",
    "    test_set = get_photo_ids(test_dress, id_to_photo_clean)\n",
    "    train_set = get_photo_ids(train_dress, id_to_photo_clean)\n",
    "    result_set = test_set|train_set\n",
    "\n",
    "    with open(PHOTO_FILE, \"w\") as new_ph_file:\n",
    "        with open(\"../data/photos/photos.txt\") as ph_file:\n",
    "            for line in ph_file:\n",
    "                id = int(line.split(\",\")[0])\n",
    "                if id in result_set:\n",
    "                    new_ph_file.write(line)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "download photos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-10-29ab3a227978>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-10-29ab3a227978>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    python download.py --urls data/photos/photos_dress.txt\u001b[0m\n\u001b[0m                  ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "!python download.py --urls data/photos/photos_dress.txt"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "get all available images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path as path\n",
    "\n",
    "PHOTO_FOLDER = \"../data/images\"\n",
    "\n",
    "ALL_IMAGES  = [int(name.split(\".\")[0]) for name in os.listdir(PHOTO_FOLDER) if path.isfile(PHOTO_FOLDER+\"/\"+name)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "form target dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "PHOTO_FOLDER = \"../data/images\"\n",
    "META_FOLDER = \"../data/meta\"\n",
    "from torch.utils.data import Dataset\n",
    "import random\n",
    "from skimage import io, transform\n",
    "\n",
    "IMG_SIZE =299\n",
    "\n",
    "def default_image_loader(path, crop=None,size=None):\n",
    "    if size is None:\n",
    "        size = (IMG_SIZE, IMG_SIZE)\n",
    "    image=Image.open(path).convert('RGB')\n",
    "    if crop is not None:\n",
    "        image = image.crop(crop)\n",
    "    image=image.resize(size)\n",
    "    return image\n",
    "\n",
    "def get_negative(positive, items):\n",
    "    while True:\n",
    "        rand_item = random.choice(items)\n",
    "        if positive != rand_item:\n",
    "            return rand_item\n",
    "    \n",
    "\n",
    "def get_image(photo_id, folder=PHOTO_FOLDER, crop=None,transform=None):\n",
    "    photo_name = str(photo_id)\n",
    "    zeros = \"0\"*(9- len(photo_name))\n",
    "    extension = \".jpeg\", \".png\", \".gif\"\n",
    "    full_photo_name = None\n",
    "    for ext in extension:\n",
    "        full_photo_name = folder+\"/\"+zeros+photo_name+ext\n",
    "        if path.isfile(full_photo_name):\n",
    "            break\n",
    "    else:\n",
    "        raise Exception(\"No Image found\")\n",
    "    image = default_image_loader(full_photo_name, crop=crop)\n",
    "    if transform is None:\n",
    "        return image\n",
    "    else:\n",
    "        return transform(image)\n",
    "\n",
    "class DressDataset(Dataset):\n",
    "    \"\"\"street2shop dress dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, photos_folder_name=PHOTO_FOLDER, transform = None):\n",
    "        self.triplets = None\n",
    "        self.retrieval = None\n",
    "        self.photos_folder = photos_folder_name\n",
    "        self.transform = transform\n",
    "    \n",
    "    def parse_triplets(self, triplets_file_name, retrieval_file_name=DRESS_FOLDER+ID_NAME):\n",
    "        self.triplets = read_json(triplets_file_name)\n",
    "        self.retrieval = read_json(retrieval_file_name)\n",
    "        self.retrieval ={it[\"product\"]:it[\"photo\"] for it in self.retrieval}\n",
    "        all_photos = [item[\"photo\"] for item in self.triplets]\n",
    "        for item in self.triplets:\n",
    "            positive_photo = self.retrieval[item[\"product\"]]\n",
    "            anchor_photo = item[\"photo\"]\n",
    "            if positive_photo not in ALL_IMAGES or anchor_photo not in ALL_IMAGES:\n",
    "                continue\n",
    "            item[\"positive\"] = positive_photo\n",
    "            item[\"anchor\"] = anchor_photo\n",
    "            del item[\"product\"]\n",
    "            del item[\"photo\"]\n",
    "            negative = get_negative(item[\"positive\"], ALL_IMAGES)\n",
    "            item[\"negative\"] = negative\n",
    "        \n",
    "        for item in self.triplets[:]:\n",
    "            if \"positive\" not in item:\n",
    "                self.triplets.remove(item)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.triplets)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        sample = {}\n",
    "        triplet = self.triplets[index]\n",
    "        sample[\"positive\"] = get_image(triplet[\"positive\"], transform = self.transform)\n",
    "        sample[\"negative\"] = get_image(triplet[\"negative\"], transform = self.transform)\n",
    "        bbox = triplet[\"bbox\"]\n",
    "        anchor = get_image(triplet[\"anchor\"], crop=(bbox[\"left\"],bbox[\"top\"],bbox[\"left\"]+bbox[\"width\"],bbox[\"top\"]+bbox[\"height\"]),\n",
    "                          transform=self.transform)\n",
    "        sample[\"anchor\"] = anchor\n",
    "        return ((sample[\"positive\"],triplet[\"positive\"]), (sample[\"anchor\"],triplet[\"anchor\"]),\n",
    "                (sample[\"negative\"], triplet[\"negative\"]))\n",
    "    \n",
    "    def get_sliced_copy(self, start, end):\n",
    "        copied = DressDataset(photos_folder_name=self.photos_folder, transform=self.transform)\n",
    "        copied.triplets = self.triplets[start:end]\n",
    "        return copied\n",
    "    \n",
    "    def resample(embeddings, margin):\n",
    "        new_triplets = []\n",
    "        for pos, anc, neg in self.triplets:\n",
    "            pos_dist = torch.dist(embeddings[pos], embeddings[anc])\n",
    "            cand = []\n",
    "            for i in embeddings:\n",
    "                if i not in (pos, anc):\n",
    "                    neg_dist = torch.dist(embeddings[anc], embeddings[i])\n",
    "                    if pos_dist - neg_dist+margin >0:\n",
    "                        cand.append(i)\n",
    "            if cand:\n",
    "                neg_cand = random.choice(cand)\n",
    "            else:\n",
    "                neg_cand = neg\n",
    "            new_triplets.append(pos, anc, neg_cand)\n",
    "        \n",
    "        sel.triplets = new_triplets\n",
    "                    \n",
    "    \n",
    "    @classmethod\n",
    "    def get_parsed_dataset(cls, triplets_file_name, photos_folder_name=PHOTO_FOLDER, \n",
    "                 retrieval_file_name=DRESS_FOLDER+ID_NAME, transform = None):\n",
    "        ds = DressDataset(photos_folder_name=photos_folder_name, transform=transform)\n",
    "        ds.parse_triplets(triplets_file_name=triplets_file_name, retrieval_file_name=retrieval_file_name)\n",
    "        return ds    \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = DressDataset.get_parsed_dataset(triplets_file_name=DRESS_FOLDER+TRAIN_NAME,\n",
    "                     transform=torch_transform.Compose([\n",
    "                                               torch_transform.ToTensor()\n",
    "                                           ]))\n",
    "test = DressDataset.get_parsed_dataset(triplets_file_name=DRESS_FOLDER+TRAIN_NAME,\n",
    "                   transform=torch_transform.Compose([\n",
    "                                               torch_transform.ToTensor()\n",
    "                                           ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "VALIDATION_PROPORTION = 0.5\n",
    "val_prop = int(len(test)*VALIDATION_PROPORTION)\n",
    "validation = test.get_sliced_copy(0, val_prop)\n",
    "final_test = test.get_sliced_copy(val_prop,len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Load dataset into model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "BATCH_SIZE = 10\n",
    "\n",
    "cuda = torch.cuda.is_available()\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if cuda else {}\n",
    "train_dataset_loader = torch.utils.data.DataLoader(train,batch_size=BATCH_SIZE, shuffle=True,**kwargs)\n",
    "test_dataset_loader = torch.utils.data.DataLoader(test,batch_size=BATCH_SIZE, shuffle=True,**kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>define the model</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "\n",
    "\n",
    "class BasisNet(nn.Module):\n",
    "    def __init__(self, embedding_size=128):\n",
    "        super(BasisNet, self).__init__()\n",
    "        self.inception = models.inception_v3(pretrained=True)\n",
    "        for param in self.inception.parameters():\n",
    "            param.requires_grad=False\n",
    "        num_ftrs = self.inception.fc.in_features\n",
    "        self.inception.fc = nn.Linear(num_ftrs, 512)\n",
    "        self.elu = nn.ELU()\n",
    "        self.final = nn.Linear(512, embedding_size)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x = self.inception(x)[0]\n",
    "        x = self.elu(x)\n",
    "        x = self.final(x)\n",
    "        return self.l2_norm(x)\n",
    "    \n",
    "    def l2_norm(self,inp):\n",
    "        input_size = inp.size()\n",
    "        buffer = torch.pow(inp, 2)\n",
    "\n",
    "        normp = torch.sum(buffer, 1).add_(1e-10)\n",
    "        norm = torch.sqrt(normp)\n",
    "\n",
    "        _output = torch.div(inp, norm.view(-1, 1).expand_as(inp))\n",
    "\n",
    "        output = _output.view(input_size)\n",
    "\n",
    "        return output\n",
    "        \n",
    "class TripletNet(nn.Module):\n",
    "    def __init__(self, embedding_net):\n",
    "        super(TripletNet, self).__init__()\n",
    "        self.embedding_net = embedding_net\n",
    "\n",
    "    def forward(self, positive, anchor, negative):\n",
    "        output1 = self.embedding_net(positive)\n",
    "        output2 = self.embedding_net(anchor)\n",
    "        output3 = self.embedding_net(negative)\n",
    "        return output1, output2, output3\n",
    "\n",
    "    def get_embedding(self, x):\n",
    "        return self.embedding_net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_func(anchor, positive, negative, size_average=True, margin = 0.3):\n",
    "    distance_positive = (anchor - positive).pow(2).sum(1)  # .pow(.5)\n",
    "    distance_negative = (anchor - negative).pow(2).sum(1)  # .pow(.5)\n",
    "    losses = F.relu(distance_positive - distance_negative + margin)\n",
    "    return losses.mean() if size_average else losses.sum()\n",
    "\n",
    "class TripletLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Triplet loss\n",
    "    Takes embeddings of an anchor sample, a positive sample and a negative sample\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, margin):\n",
    "        super(TripletLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, anchor, positive, negative, size_average=True):\n",
    "        return loss_func(anchor, positive, negative, size_average, self.margin)\n",
    "    \n",
    "dress_net = TripletNet(BasisNet())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 before resampling\n",
      "1 before resampling\n",
      "2 before resampling\n",
      "3 before resampling\n",
      "4 before resampling\n",
      "5 before resampling\n",
      "6 before resampling\n",
      "7 before resampling\n",
      "8 before resampling\n",
      "9 before resampling\n",
      "10 before resampling\n",
      "11 before resampling\n",
      "12 before resampling\n",
      "13 before resampling\n",
      "14 before resampling\n",
      "15 before resampling\n",
      "16 before resampling\n",
      "17 before resampling\n",
      "18 before resampling\n",
      "19 before resampling\n",
      "20 before resampling\n",
      "21 before resampling\n",
      "22 before resampling\n",
      "23 before resampling\n",
      "24 before resampling\n",
      "25 before resampling\n",
      "26 before resampling\n",
      "27 before resampling\n",
      "28 before resampling\n",
      "29 before resampling\n",
      "30 before resampling\n",
      "31 before resampling\n",
      "32 before resampling\n",
      "33 before resampling\n",
      "34 before resampling\n",
      "35 before resampling\n",
      "36 before resampling\n",
      "37 before resampling\n",
      "38 before resampling\n",
      "39 before resampling\n",
      "40 before resampling\n",
      "41 before resampling\n",
      "42 before resampling\n",
      "43 before resampling\n",
      "44 before resampling\n",
      "45 before resampling\n",
      "46 before resampling\n",
      "47 before resampling\n",
      "48 before resampling\n",
      "49 before resampling\n",
      "50 before resampling\n",
      "51 before resampling\n",
      "52 before resampling\n",
      "53 before resampling\n",
      "54 before resampling\n",
      "55 before resampling\n",
      "56 before resampling\n",
      "57 before resampling\n",
      "58 before resampling\n",
      "59 before resampling\n",
      "60 before resampling\n",
      "61 before resampling\n",
      "62 before resampling\n",
      "63 before resampling\n",
      "64 before resampling\n",
      "65 before resampling\n",
      "66 before resampling\n",
      "67 before resampling\n",
      "68 before resampling\n",
      "69 before resampling\n",
      "70 before resampling\n",
      "71 before resampling\n",
      "72 before resampling\n",
      "73 before resampling\n",
      "74 before resampling\n",
      "75 before resampling\n",
      "76 before resampling\n",
      "77 before resampling\n",
      "78 before resampling\n",
      "79 before resampling\n",
      "80 before resampling\n",
      "81 before resampling\n",
      "82 before resampling\n",
      "83 before resampling\n",
      "84 before resampling\n",
      "85 before resampling\n",
      "86 before resampling\n",
      "87 before resampling\n",
      "88 before resampling\n",
      "89 before resampling\n",
      "90 before resampling\n",
      "91 before resampling\n",
      "92 before resampling\n",
      "93 before resampling\n",
      "94 before resampling\n",
      "95 before resampling\n",
      "96 before resampling\n",
      "97 before resampling\n",
      "98 before resampling\n",
      "99 before resampling\n",
      "100 before resampling\n",
      "101 before resampling\n",
      "102 before resampling\n",
      "103 before resampling\n",
      "104 before resampling\n",
      "105 before resampling\n",
      "106 before resampling\n",
      "107 before resampling\n",
      "108 before resampling\n",
      "109 before resampling\n",
      "110 before resampling\n",
      "111 before resampling\n",
      "112 before resampling\n",
      "113 before resampling\n",
      "114 before resampling\n",
      "115 before resampling\n",
      "116 before resampling\n",
      "117 before resampling\n",
      "118 before resampling\n",
      "119 before resampling\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda2/lib/python2.7/site-packages/PIL/Image.py:2514: DecompressionBombWarning: Image size (123871510 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  DecompressionBombWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120 before resampling\n",
      "121 before resampling\n",
      "122 before resampling\n",
      "123 before resampling\n",
      "124 before resampling\n",
      "125 before resampling\n",
      "126 before resampling\n",
      "127 before resampling\n",
      "128 before resampling\n",
      "129 before resampling\n",
      "130 before resampling\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-91-a15906d1c6dd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mpositive\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manchor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnegative\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mvpositive\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvanchor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvnegative\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpositive\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manchor\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnegative\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mp_emb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ma_emb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn_emd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdress_net\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvanchor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvpositive\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvnegative\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0membeddings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpositive\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mp_emb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda2/lib/python2.7/site-packages/torch/nn/modules/module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    355\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 357\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    358\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-87-c6c0d66035d3>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, positive, anchor, negative)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpositive\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manchor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnegative\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m         \u001b[0moutput1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding_net\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpositive\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m         \u001b[0moutput2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding_net\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manchor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0moutput3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding_net\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnegative\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda2/lib/python2.7/site-packages/torch/nn/modules/module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    355\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 357\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    358\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-87-c6c0d66035d3>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0melu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda2/lib/python2.7/site-packages/torch/nn/modules/module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    355\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 357\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    358\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda2/lib/python2.7/site-packages/torchvision-0.1.9-py2.7.egg/torchvision/models/inception.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     75\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m0.225\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m0.406\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0;31m# 299 x 299 x 3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConv2d_1a_3x3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m         \u001b[0;31m# 149 x 149 x 32\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConv2d_2a_3x3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda2/lib/python2.7/site-packages/torch/nn/modules/module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    355\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 357\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    358\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda2/lib/python2.7/site-packages/torchvision-0.1.9-py2.7.egg/torchvision/models/inception.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    323\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 325\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    326\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda2/lib/python2.7/site-packages/torch/nn/modules/module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    355\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 357\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    358\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda2/lib/python2.7/site-packages/torch/nn/modules/batchnorm.pyc\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     35\u001b[0m         return F.batch_norm(\n\u001b[1;32m     36\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunning_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunning_var\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m             self.training, self.momentum, self.eps)\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda2/lib/python2.7/site-packages/torch/nn/functional.pyc\u001b[0m in \u001b[0;36mbatch_norm\u001b[0;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[1;32m   1011\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Expected more than 1 value per channel when training, got input size {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1012\u001b[0m     \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_functions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBatchNorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrunning_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrunning_var\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcudnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menabled\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1013\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1014\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1015\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.Adam(filter(lambda p: p.requires_grad,dress_net.parameters()), lr=0.0001)\n",
    "criterion = TripletLoss(0.3) \n",
    "EPOCH = 2\n",
    "for i in range(EPOCH):\n",
    "    losses = []\n",
    "    embeddings={}\n",
    "    for batch_i, (positive, anchor, negative) in enumerate(train_dataset_loader):\n",
    "        vpositive, vanchor, vnegative = Variable(positive[0]), Variable(anchor[0]), Variable(negative[0])\n",
    "        p_emb,a_emb,n_emd = dress_net(vanchor, vpositive, vnegative)\n",
    "        for i in range(BATCH_SIZE):\n",
    "            embeddings[positive[1][i]] =p_emb[i]\n",
    "            embeddings[anchor[1][i]] = a_emb[i]\n",
    "            embeddings[negative[1][i]] = n_emd[i]\n",
    "        print str(batch_i)+\" before resampling\"\n",
    "    \n",
    "    train.resample(embeddings,\n",
    "                   margin = 0.3)\n",
    "        \n",
    "    for batch_i, (positive, anchor, negative) in enumerate(train_dataset_loader):\n",
    "        vpositive, vanchor, vnegative = Variable(positive[0]), Variable(anchor[0]), Variable(negative[0])\n",
    "        p_emb,a_emb,n_emd = dress_net(vanchor, vpositive, vnegative)\n",
    "        loss = criterion(p,a,n)\n",
    "        losses.append(loss.data[0])\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print str(batch_i)+\" after resampling\"\n",
    "        \n",
    "    print np.mean(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anaconda2]",
   "language": "python",
   "name": "conda-env-anaconda2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
