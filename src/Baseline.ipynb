{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <h1>add required imports and constants</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.15.0'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext line_profiler\n",
    "\n",
    "import json\n",
    "import os\n",
    "import os.path as path\n",
    "import random\n",
    "import six\n",
    "\n",
    "\n",
    "\n",
    "from annoy import AnnoyIndex\n",
    "import numpy as np\n",
    "from torchvision import transforms as torch_transform\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "from PIL import ImageFile\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "PHOTO_FOLDER = \"../data/images\"\n",
    "META_FOLDER = \"../data/\"\n",
    "DRESS_FOLDER = \"../data/json/\"\n",
    "\n",
    "\n",
    "ID_NAME = \"retrieval_dresses.json\"\n",
    "TEST_NAME = \"test_pairs_dresses.json\"\n",
    "TRAIN_NAME = \"train_pairs_dresses.json\"\n",
    "PHOTO_FILE = \"../data/photos/photos.txt\"\n",
    "PHOTO_DRESS_FILE = \"../data/photos/photos_dress.txt\"\n",
    "SEED = 17\n",
    "EPOCH = 100\n",
    "MARGIN = 0.3\n",
    "CHECKPOINT_FILE = \"checkpoint.pth.tar\"\n",
    "IMG_SIZE =224\n",
    "BATCH_SIZE = 10\n",
    "HAS_CUDA = torch.cuda.is_available()\n",
    "LR = 0.0001\n",
    "\n",
    "\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "FINAL_EMBEDDING_SIZE = 128\n",
    "\n",
    "def read_json(file_name):\n",
    "    with open(file_name) as f:\n",
    "        obj = json.loads(f.readline())\n",
    "    return obj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>download dataset metadata and photos info(optional)</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'street2shop'...\n",
      "remote: Counting objects: 9, done.\u001b[K\n",
      "remote: Compressing objects: 100% (7/7), done.\u001b[K\n",
      "remote: Total 9 (delta 1), reused 9 (delta 1), pack-reused 0\u001b[K\n",
      "Unpacking objects: 100% (9/9), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/pumpikano/street2shop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir street2shop/images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 2777k  100 2777k    0     0   297k      0  0:00:09  0:00:09 --:--:--  346k\n"
     ]
    }
   ],
   "source": [
    "!curl -O http://www.tamaraberg.com/street2shop/wheretobuyit/meta.zip\n",
    "!curl http://www.tamaraberg.com/street2shop/wheretobuyit/photos.tar | tar x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  meta.zip\n",
      "   creating: meta/\n",
      "  inflating: meta/.DS_Store          \n",
      "   creating: __MACOSX/\n",
      "   creating: __MACOSX/meta/\n",
      "  inflating: __MACOSX/meta/._.DS_Store  \n",
      "   creating: meta/json/\n",
      "  inflating: meta/json/.DS_Store     \n",
      "   creating: __MACOSX/meta/json/\n",
      "  inflating: __MACOSX/meta/json/._.DS_Store  \n",
      "  inflating: meta/json/retrieval_bags.json  \n",
      "  inflating: meta/json/retrieval_belts.json  \n",
      "  inflating: meta/json/retrieval_dresses.json  \n",
      "  inflating: meta/json/retrieval_eyewear.json  \n",
      "  inflating: meta/json/retrieval_footwear.json  \n",
      "  inflating: meta/json/retrieval_hats.json  \n",
      "  inflating: meta/json/retrieval_leggings.json  \n",
      "  inflating: meta/json/retrieval_outerwear.json  \n",
      "  inflating: meta/json/retrieval_pants.json  \n",
      "  inflating: meta/json/retrieval_skirts.json  \n",
      "  inflating: meta/json/retrieval_tops.json  \n",
      "  inflating: meta/json/test_pairs_bags.json  \n",
      "  inflating: meta/json/test_pairs_belts.json  \n",
      "  inflating: meta/json/test_pairs_dresses.json  \n",
      "  inflating: meta/json/test_pairs_eyewear.json  \n",
      "  inflating: meta/json/test_pairs_footwear.json  \n",
      "  inflating: meta/json/test_pairs_hats.json  \n",
      "  inflating: meta/json/test_pairs_leggings.json  \n",
      "  inflating: meta/json/test_pairs_outerwear.json  \n",
      "  inflating: meta/json/test_pairs_pants.json  \n",
      "  inflating: meta/json/test_pairs_skirts.json  \n",
      "  inflating: meta/json/test_pairs_tops.json  \n",
      "  inflating: meta/json/train_pairs_bags.json  \n",
      "  inflating: meta/json/train_pairs_belts.json  \n",
      "  inflating: meta/json/train_pairs_dresses.json  \n",
      "  inflating: meta/json/train_pairs_eyewear.json  \n",
      "  inflating: meta/json/train_pairs_footwear.json  \n",
      "  inflating: meta/json/train_pairs_hats.json  \n",
      "  inflating: meta/json/train_pairs_leggings.json  \n",
      "  inflating: meta/json/train_pairs_outerwear.json  \n",
      "  inflating: meta/json/train_pairs_pants.json  \n",
      "  inflating: meta/json/train_pairs_skirts.json  \n",
      "  inflating: meta/json/train_pairs_tops.json  \n",
      "  inflating: meta/README.txt         \n",
      "  inflating: __MACOSX/meta/._README.txt  \n"
     ]
    }
   ],
   "source": [
    "!unzip meta.zip\n",
    "!rm meta.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘../data/images’: File exists\r\n"
     ]
    }
   ],
   "source": [
    "!mkdir ../data\n",
    "!mkdir ../data/images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_photo_ids(obj, retr_dict):\n",
    "    photos = set()\n",
    "    for item in obj:\n",
    "        photos.add(item[\"photo\"])\n",
    "        product_photos = retr_dict[item[\"product\"]]\n",
    "        photos.update(product_photos)\n",
    "    return photos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    id_to_photo_dirty = read_json(DRESS_FOLDER+ID_NAME)\n",
    "    id_to_photo_clean = {}\n",
    "    for item in id_to_photo_dirty:\n",
    "        if item[\"product\"] in id_to_photo_clean:\n",
    "            id_to_photo_clean[item[\"product\"]].append(item[\"photo\"])\n",
    "        else:\n",
    "            id_to_photo_clean[item[\"product\"]] = [item[\"photo\"]]\n",
    "    test_dress = read_json(DRESS_FOLDER + TEST_NAME)\n",
    "    train_dress = read_json(DRESS_FOLDER + TRAIN_NAME)\n",
    "    test_set = get_photo_ids(test_dress, id_to_photo_clean)\n",
    "    train_set = get_photo_ids(train_dress, id_to_photo_clean)\n",
    "    result_set = test_set|train_set\n",
    "\n",
    "    with open(PHOTO_DRESS_FILE, \"w\") as new_ph_file:\n",
    "        with open(PHOTO_FILE) as ph_file:\n",
    "            for line in ph_file:\n",
    "                id = int(line.split(\",\")[0])\n",
    "                if id in result_set:\n",
    "                    new_ph_file.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\r\n",
      "Error in atexit._run_exitfuncs:\r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"/home/snake/anaconda3/envs/myenv/lib/python3.6/concurrent/futures/thread.py\", line 40, in _python_exit\r\n",
      "    t.join()\r\n",
      "  File \"/home/snake/anaconda3/envs/myenv/lib/python3.6/threading.py\", line 1056, in join\r\n",
      "    self._wait_for_tstate_lock()\r\n",
      "  File \"/home/snake/anaconda3/envs/myenv/lib/python3.6/threading.py\", line 1072, in _wait_for_tstate_lock\r\n",
      "    elif lock.acquire(block, timeout):\r\n",
      "KeyboardInterrupt\r\n"
     ]
    }
   ],
   "source": [
    "!python street2shop/download.py --urls photos/photos_dress.txt --image_dir ../../data/images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> prepare datasets </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALL_IMAGES  = [int(name.split(\".\")[0]) for name in os.listdir(PHOTO_FOLDER) if path.isfile(PHOTO_FOLDER+\"/\"+name)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def default_image_loader(path, crop=None,size=None):\n",
    "    if size is None:\n",
    "        size = (IMG_SIZE, IMG_SIZE)\n",
    "    image=Image.open(path).convert('RGB')\n",
    "    if crop is not None:\n",
    "        image = image.crop(crop)\n",
    "    image=image.resize(size)\n",
    "    return image\n",
    "\n",
    "def get_negative(positive, items):\n",
    "    while True:\n",
    "        rand_item = random.choice(items)\n",
    "        if positive != rand_item:\n",
    "            return rand_item\n",
    "    \n",
    "\n",
    "def get_image(photo_id, folder=PHOTO_FOLDER, crop=None,transform=None):\n",
    "    photo_name = str(photo_id)\n",
    "    zeros = \"0\"*(9- len(photo_name))\n",
    "    extension = \".jpeg\", \".png\", \".gif\"\n",
    "    full_photo_name = None\n",
    "    for ext in extension:\n",
    "        full_photo_name = folder+\"/\"+zeros+photo_name+ext\n",
    "        if path.isfile(full_photo_name):\n",
    "            break\n",
    "    else:\n",
    "        raise Exception(\"No Image found\")\n",
    "    image = default_image_loader(full_photo_name, crop=crop)\n",
    "    if transform is None:\n",
    "        return image\n",
    "    else:\n",
    "        return transform(image)\n",
    "\n",
    "class DressDataset(Dataset):\n",
    "    \"\"\"street2shop dress dataset.\"\"\"\n",
    "    \n",
    "    cach = {}\n",
    "    \n",
    "    def __init__(self, photos_folder_name=PHOTO_FOLDER, transform = None):\n",
    "        self.triplets = None\n",
    "        self.retrieval = None\n",
    "        self.photos_folder_name = photos_folder_name\n",
    "        self.transform = transform\n",
    "    \n",
    "    def parse_triplets(self, triplets_file_name, retrieval_file_name=DRESS_FOLDER+ID_NAME):\n",
    "        self.triplets = read_json(triplets_file_name)\n",
    "        self.retrieval = read_json(retrieval_file_name)\n",
    "        self.retrieval ={it[\"product\"]:it[\"photo\"] for it in self.retrieval}\n",
    "        all_photos = [item[\"photo\"] for item in self.triplets]\n",
    "        for item in self.triplets:\n",
    "            positive_photo = self.retrieval[item[\"product\"]]\n",
    "            anchor_photo = item[\"photo\"]\n",
    "            if positive_photo not in ALL_IMAGES or anchor_photo not in ALL_IMAGES:\n",
    "                continue\n",
    "            item[\"positive\"] = positive_photo\n",
    "            item[\"anchor\"] = anchor_photo\n",
    "            del item[\"product\"]\n",
    "            del item[\"photo\"]\n",
    "            negative = get_negative(item[\"positive\"], ALL_IMAGES)\n",
    "            item[\"negative\"] = negative\n",
    "        \n",
    "        for item in self.triplets[:]:\n",
    "            if \"positive\" not in item:\n",
    "                self.triplets.remove(item)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.triplets)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        sample = {}\n",
    "        triplet = self.triplets[index]\n",
    "        sample[\"positive\"] = get_image(triplet[\"positive\"], transform = self.transform)\n",
    "        sample[\"negative\"] = get_image(triplet[\"negative\"], transform = self.transform)\n",
    "        bbox = triplet[\"bbox\"]\n",
    "        anchor = get_image(triplet[\"anchor\"], crop=(bbox[\"left\"],bbox[\"top\"],bbox[\"left\"]+bbox[\"width\"],bbox[\"top\"]+bbox[\"height\"]),\n",
    "                          transform=self.transform)\n",
    "        sample[\"anchor\"] = anchor\n",
    "        return ((sample[\"positive\"],triplet[\"positive\"]), (sample[\"anchor\"],triplet[\"anchor\"]),\n",
    "                (sample[\"negative\"], triplet[\"negative\"]))\n",
    "    \n",
    "    def get_sliced_copy(self, start, end):\n",
    "        copied = DressDataset(photos_folder_name=self.photos_folder_name, transform=self.transform)\n",
    "        copied.triplets = self.triplets[start:end]\n",
    "        return copied\n",
    "    \n",
    "    def resample(self, embeddings, mapping, margin):\n",
    "        new_triplets = []\n",
    "        reverse_mapping = {v:k for k,v in six.iteritems(mapping)}\n",
    "        for triplet in self.triplets:\n",
    "            pos, anc, neg = mapping[triplet[\"positive\"]], mapping[triplet[\"anchor\"]], mapping[triplet[\"negative\"]]\n",
    "            dists = np.linalg.norm(embeddings - embeddings[anc], axis = 1)\n",
    "            delta = dists[pos] - dists\n",
    "            semihard_cands = np.squeeze(np.where(np.logical_and(delta>0, delta<margin)))\n",
    "            if semihard_cands.size:\n",
    "                while True:\n",
    "                    neg_cand = np.random.choice(semihard_cands)\n",
    "                    if neg_cand not in (pos, anc):\n",
    "                        break\n",
    "            else:\n",
    "                neg_cand = neg\n",
    "            triplet[\"negative\"] = reverse_mapping[neg_cand]\n",
    "            new_triplets.append(triplet)\n",
    "        \n",
    "        self.triplets = new_triplets\n",
    "    \n",
    "    def state_dict(self):\n",
    "        state = {}\n",
    "        keys = [\"photos_folder_name\", \"triplets\",\"retrieval\"]\n",
    "        for k in keys:\n",
    "            state[k] = getattr(self,k)\n",
    "        return state\n",
    "    \n",
    "    @classmethod\n",
    "    def load(cls, state_dict, transform):\n",
    "        instance = cls(photos_folder_name=state_dict[\"photos_folder_name\"], transform=transform)\n",
    "        instance.retrieval = state_dict[\"retrieval\"]\n",
    "        instance.triplets = state_dict[\"triplets\"]\n",
    "        return instance\n",
    "                                                     \n",
    "    @classmethod\n",
    "    def get_parsed_dataset(cls, triplets_file_name, photos_folder_name=PHOTO_FOLDER, \n",
    "                 retrieval_file_name=DRESS_FOLDER+ID_NAME, transform = None):\n",
    "        ds = DressDataset(photos_folder_name=photos_folder_name, transform=transform)\n",
    "        ds.parse_triplets(triplets_file_name=triplets_file_name, retrieval_file_name=retrieval_file_name)\n",
    "        return ds  \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> dataset testing</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_train = DressDataset.get_parsed_dataset(triplets_file_name=DRESS_FOLDER+TRAIN_NAME,\n",
    "                     transform=torch_transform.Compose([\n",
    "                                               torch_transform.ToTensor()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/snake/anaconda3/envs/myenv/lib/python3.6/site-packages/numpy/linalg/linalg.py:2378: RuntimeWarning: invalid value encountered in sqrt\n",
      "  return sqrt(add.reduce(s, axis=axis, keepdims=keepdims))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 55s, sys: 968 ms, total: 1min 56s\n",
      "Wall time: 30.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "fake_mappings = {}\n",
    "counter=0\n",
    "for triplet in fake_train.triplets:\n",
    "            pos, anc, neg = triplet[\"positive\"], triplet[\"anchor\"], triplet[\"negative\"]\n",
    "            if pos not in fake_mappings:\n",
    "                fake_mappings[pos] = counter\n",
    "                counter = counter+1\n",
    "            if anc not in fake_mappings:\n",
    "                fake_mappings[anc] = counter\n",
    "                counter = counter+1\n",
    "            if neg not in fake_mappings:\n",
    "                fake_mappings[neg] = counter\n",
    "                counter = counter+1\n",
    "if HAS_CUDA:\n",
    "    fake_embeddings = torch.randn(len(fake_mappings),FINAL_EMBEDDING_SIZE).cpu().data.numpy()\n",
    "else:\n",
    "    fake_embeddings = torch.randn(len(fake_mappings),FINAL_EMBEDDING_SIZE).numpy()\n",
    "%lprun -f fake_train.resample fake_train.resample(fake_embeddings, fake_mappings, margin=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = DressDataset.get_parsed_dataset(triplets_file_name=DRESS_FOLDER+TRAIN_NAME,\n",
    "                     transform=torch_transform.Compose([\n",
    "                                               torch_transform.ToTensor()\n",
    "                                           ]))\n",
    "test = DressDataset.get_parsed_dataset(triplets_file_name=DRESS_FOLDER+TRAIN_NAME,\n",
    "                   transform=torch_transform.Compose([\n",
    "                                               torch_transform.ToTensor()\n",
    "                                           ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "micro_train  = train.get_sliced_copy(0,500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "VALIDATION_PROPORTION = 0.5\n",
    "val_prop = int(len(test)*VALIDATION_PROPORTION)\n",
    "validation = test.get_sliced_copy(0, val_prop)\n",
    "final_test = test.get_sliced_copy(val_prop,len(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>define the model</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasisNet(nn.Module):\n",
    "    def __init__(self, embedding_size=FINAL_EMBEDDING_SIZE):\n",
    "        super(BasisNet, self).__init__()\n",
    "        self.inception = models.resnet18(pretrained=True)\n",
    "        for param in self.inception.parameters():\n",
    "            param.requires_grad=False\n",
    "        num_ftrs = self.inception.fc.in_features\n",
    "        self.inception.fc = nn.Linear(num_ftrs, 512)\n",
    "        self.elu = nn.ELU()\n",
    "        self.final = nn.Linear(512, embedding_size)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x = self.inception(x)\n",
    "        x = self.elu(x)\n",
    "        x = self.final(x)\n",
    "        return self.l2_norm(x)\n",
    "    \n",
    "    def l2_norm(self,inp):\n",
    "        input_size = inp.size()\n",
    "        buffer = torch.pow(inp, 2)\n",
    "\n",
    "        normp = torch.sum(buffer, 1).add_(1e-10)\n",
    "        norm = torch.sqrt(normp)\n",
    "\n",
    "        _output = torch.div(inp, norm.view(-1, 1).expand_as(inp))\n",
    "\n",
    "        output = _output.view(input_size)\n",
    "\n",
    "        return output\n",
    "        \n",
    "class TripletNet(nn.Module):\n",
    "    def __init__(self, embedding_net):\n",
    "        super(TripletNet, self).__init__()\n",
    "        self.embedding_net = embedding_net\n",
    "\n",
    "    def forward(self, positive, anchor, negative):\n",
    "        output1 = self.embedding_net(positive)\n",
    "        output2 = self.embedding_net(anchor)\n",
    "        output3 = self.embedding_net(negative)\n",
    "        return output1, output2, output3\n",
    "\n",
    "    def get_embedding(self, x):\n",
    "        return self.embedding_net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "if HAS_CUDA:\n",
    "    cuda = torch.device('cuda') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_func(anchor, positive, negative, size_average=True, margin = 0.3):\n",
    "    distance_positive = (anchor - positive).pow(2).sum(1)  # .pow(.5)\n",
    "    distance_negative = (anchor - negative).pow(2).sum(1)  # .pow(.5)\n",
    "    losses = F.relu(distance_positive - distance_negative + margin)\n",
    "    return losses.mean() if size_average else losses.sum()\n",
    "\n",
    "class TripletLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Triplet loss\n",
    "    Takes embeddings of an anchor sample, a positive sample and a negative sample\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, margin):\n",
    "        super(TripletLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, anchor, positive, negative, size_average=True):\n",
    "        return loss_func(anchor, positive, negative, size_average, self.margin)\n",
    "    \n",
    "dress_net = TripletNet(BasisNet())\n",
    "if HAS_CUDA:\n",
    "    dress_net.to(cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_embedding(mapping, embeddings, emb_to_save, index, annoy):\n",
    "    if index not in mapping:\n",
    "        embeddings.append(emb_to_save)\n",
    "        new_index = len(embeddings) - 1\n",
    "        mapping[index] = new_index\n",
    "        if HAS_CUDA:\n",
    "            annoy.add_item(new_index, emb_to_save.cpu().data.numpy())\n",
    "        else:\n",
    "            annoy.add_item(new_index, emb_to_save.numpy())\n",
    "        return new_index\n",
    "    else:\n",
    "        return mapping[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(state, filename=CHECKPOINT_FILE):\n",
    "    torch.save(state, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(model, train_dataset, batch_size=BATCH_SIZE, lr=LR, margin=MARGIN, n_epoch=EPOCH, save=False, validation_dataset = None):\n",
    "    optimizer = optim.Adam(filter(lambda p: p.requires_grad,model.parameters()), lr=lr)\n",
    "    kwargs = {'num_workers': 10, 'pin_memory': True} if HAS_CUDA else {}\n",
    "    loader = torch.utils.data.DataLoader(train_dataset,batch_size=batch_size, shuffle=False,**kwargs)\n",
    "    all_losses=[]\n",
    "    criterion = TripletLoss(margin)\n",
    "    for epoch in range(n_epoch):\n",
    "        losses = []\n",
    "        train_embeddings=[]\n",
    "        train_embedding_map = {}\n",
    "        torch.set_grad_enabled(False)\n",
    "        annoy_index = AnnoyIndex(FINAL_EMBEDDING_SIZE, metric=\"euclidean\")\n",
    "        for batch_i, (positive, anchor, negative) in enumerate(loader):\n",
    "            if HAS_CUDA:\n",
    "                vpositive, vanchor, vnegative = Variable(positive[0].pin_memory()).cuda(async=True), Variable(anchor[0].pin_memory()).cuda(async=True), Variable(negative[0].pin_memory()).cuda(async=True)\n",
    "            else:\n",
    "                vpositive, vanchor, vnegative = Variable(positive[0]), Variable(anchor[0]), Variable(negative[0])\n",
    "            p_emb,a_emb,n_emb = dress_net(vanchor, vpositive, vnegative)\n",
    "            for i in range(len(p_emb)):\n",
    "                save_embedding(train_embedding_map, train_embeddings, p_emb[i],positive[1][i].item(), annoy_index)\n",
    "                save_embedding(train_embedding_map, train_embeddings, a_emb[i],anchor[1][i].item(), annoy_index)\n",
    "                save_embedding(train_embedding_map, train_embeddings, n_emb[i],negative[1][i].item(), annoy_index)\n",
    "        if HAS_CUDA:\n",
    "            train_embeddings_ct = torch.stack(train_embeddings).cpu().data.numpy()\n",
    "        else:\n",
    "            train_embeddings_ct = torch.stack(train_embeddings).numpy()\n",
    "        train_dataset.resample(train_embeddings_ct, train_embedding_map,\n",
    "                       margin = margin)\n",
    "        torch.set_grad_enabled(True)\n",
    "        for batch_i, (positive, anchor, negative) in enumerate(loader):\n",
    "            if HAS_CUDA:\n",
    "                vpositive, vanchor, vnegative = Variable(positive[0].cuda(async=True)), Variable(anchor[0].cuda(async=True)), Variable(negative[0].cuda(async=True))\n",
    "            else:\n",
    "                vpositive, vanchor, vnegative = Variable(positive[0]), Variable(anchor[0]), Variable(negative[0])\n",
    "            p_emb,a_emb,n_emb = dress_net(vanchor, vpositive, vnegative)\n",
    "            loss = criterion(p_emb,a_emb,n_emb)\n",
    "            losses.append(loss.item())\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        score = np.mean(losses)\n",
    "        all_losses.append(score)\n",
    "        print(\"loss %s after epoch %s \"%(score, epoch))\n",
    "        if save:\n",
    "            save_checkpoint({\n",
    "                'epoch': epoch + 1,\n",
    "                'model': dress_net.state_dict(),\n",
    "                'dataset': micro_train.state_dict(),\n",
    "                'optimizer' : optimizer.state_dict(),\n",
    "            })\n",
    "        \n",
    "        if validation_dataset is not None:\n",
    "            torch.set_grad_enabled(False)\n",
    "            pairs = []\n",
    "           \n",
    "            val_loader = torch.utils.data.DataLoader(validation_dataset,batch_size=batch_size, shuffle=True,**kwargs)\n",
    "            for batch_i, (positive, anchor, negative) in enumerate(val_loader):\n",
    "                if HAS_CUDA:\n",
    "                    vpositive, vanchor, vnegative = Variable(positive[0].cuda(async=True)), Variable(anchor[0].cuda(async=True)), Variable(negative[0].cuda(async=True))\n",
    "                else:\n",
    "                    vpositive, vanchor, vnegative = Variable(positive[0]), Variable(anchor[0]), Variable(negative[0])\n",
    "                p_emb,a_emb,n_emb = dress_net(vanchor, vpositive, vnegative)\n",
    "                for i in range(len(p_emb)):\n",
    "                    pair1 = save_embedding(train_embedding_map, train_embeddings, p_emb[i],positive[1][i].item(), annoy_index)\n",
    "                    pair2 = save_embedding(train_embedding_map, train_embeddings, a_emb[i],anchor[1][i].item(), annoy_index)\n",
    "                    pairs.append((pair1, pair2))\n",
    "            annoy_index.build(50)\n",
    "            accuracy = 0.0\n",
    "            for pair in pairs:\n",
    "                neighbors = annoy_index.get_nns_by_item(pair[0], 10)\n",
    "                if pair[1] in neighbors:\n",
    "                    accuracy= accuracy+1\n",
    "            total = len(validation_dataset)\n",
    "            print(\" top 5 accuracy %s\"% str(accuracy/total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 0.053696143282577394 after epoch 0 \n",
      " top 5 accuracy 0.6\n"
     ]
    }
   ],
   "source": [
    "%lprun -f fit fit(dress_net,micro_train, n_epoch=1, validation_dataset=test.get_sliced_copy(0,100), save =True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/snake/anaconda3/envs/myenv/lib/python3.6/site-packages/PIL/Image.py:2546: DecompressionBombWarning: Image size (123871510 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  DecompressionBombWarning)\n",
      "/home/snake/anaconda3/envs/myenv/lib/python3.6/site-packages/PIL/Image.py:2546: DecompressionBombWarning: Image size (123871510 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  DecompressionBombWarning)\n",
      "/home/snake/anaconda3/envs/myenv/lib/python3.6/site-packages/numpy/linalg/linalg.py:2378: RuntimeWarning: invalid value encountered in sqrt\n",
      "  return sqrt(add.reduce(s, axis=axis, keepdims=keepdims))\n",
      "/home/snake/anaconda3/envs/myenv/lib/python3.6/site-packages/PIL/Image.py:2546: DecompressionBombWarning: Image size (123871510 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  DecompressionBombWarning)\n",
      "/home/snake/anaconda3/envs/myenv/lib/python3.6/site-packages/PIL/Image.py:2546: DecompressionBombWarning: Image size (123871510 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  DecompressionBombWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 0.09411124714362466 after epoch 0 \n",
      " top 5 accuracy 0.164\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/snake/anaconda3/envs/myenv/lib/python3.6/site-packages/PIL/Image.py:2546: DecompressionBombWarning: Image size (123871510 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  DecompressionBombWarning)\n",
      "/home/snake/anaconda3/envs/myenv/lib/python3.6/site-packages/PIL/Image.py:2546: DecompressionBombWarning: Image size (123871510 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  DecompressionBombWarning)\n",
      "/home/snake/anaconda3/envs/myenv/lib/python3.6/site-packages/PIL/Image.py:2546: DecompressionBombWarning: Image size (123871510 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  DecompressionBombWarning)\n",
      "/home/snake/anaconda3/envs/myenv/lib/python3.6/site-packages/PIL/Image.py:2546: DecompressionBombWarning: Image size (123871510 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  DecompressionBombWarning)\n",
      "/home/snake/anaconda3/envs/myenv/lib/python3.6/site-packages/PIL/Image.py:2546: DecompressionBombWarning: Image size (123871510 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  DecompressionBombWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 0.08068024022037779 after epoch 1 \n",
      " top 5 accuracy 0.112\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/snake/anaconda3/envs/myenv/lib/python3.6/site-packages/PIL/Image.py:2546: DecompressionBombWarning: Image size (123871510 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  DecompressionBombWarning)\n",
      "/home/snake/anaconda3/envs/myenv/lib/python3.6/site-packages/PIL/Image.py:2546: DecompressionBombWarning: Image size (123871510 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  DecompressionBombWarning)\n",
      "/home/snake/anaconda3/envs/myenv/lib/python3.6/site-packages/PIL/Image.py:2546: DecompressionBombWarning: Image size (123871510 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  DecompressionBombWarning)\n",
      "/home/snake/anaconda3/envs/myenv/lib/python3.6/site-packages/PIL/Image.py:2546: DecompressionBombWarning: Image size (123871510 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  DecompressionBombWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 0.0697899733054523 after epoch 2 \n",
      " top 5 accuracy 0.118\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/snake/anaconda3/envs/myenv/lib/python3.6/site-packages/PIL/Image.py:2546: DecompressionBombWarning: Image size (123871510 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  DecompressionBombWarning)\n",
      "/home/snake/anaconda3/envs/myenv/lib/python3.6/site-packages/PIL/Image.py:2546: DecompressionBombWarning: Image size (123871510 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  DecompressionBombWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 0.0617109383507752 after epoch 3 \n",
      " top 5 accuracy 0.115\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/snake/anaconda3/envs/myenv/lib/python3.6/site-packages/PIL/Image.py:2546: DecompressionBombWarning: Image size (123871510 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  DecompressionBombWarning)\n",
      "/home/snake/anaconda3/envs/myenv/lib/python3.6/site-packages/PIL/Image.py:2546: DecompressionBombWarning: Image size (123871510 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  DecompressionBombWarning)\n",
      "/home/snake/anaconda3/envs/myenv/lib/python3.6/site-packages/PIL/Image.py:2546: DecompressionBombWarning: Image size (123871510 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  DecompressionBombWarning)\n",
      "/home/snake/anaconda3/envs/myenv/lib/python3.6/site-packages/PIL/Image.py:2546: DecompressionBombWarning: Image size (123871510 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  DecompressionBombWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 0.056248353809449814 after epoch 4 \n",
      " top 5 accuracy 0.139\n",
      "CPU times: user 12min 56s, sys: 4min 38s, total: 17min 35s\n",
      "Wall time: 1h 6min 43s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "fit(dress_net,train, n_epoch=5, validation_dataset=test.get_sliced_copy(0,1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_net = TripletNet(BasisNet())\n",
    "saved_net.to(cuda)\n",
    "saved_optimizer = optim.Adam(filter(lambda p: p.requires_grad,saved_net.parameters()), lr=0.0001)\n",
    "checkpoint = torch.load('/tmp/checkpoint.pth.tar')\n",
    "saved_model = saved_net.load_state_dict(checkpoint[\"model\"])\n",
    "saved_optimizer = saved_optimizer.load_state_dict(checkpoint[\"optimizer\"])\n",
    "saved_ds = DressDataset.load(checkpoint[\"dataset\"], transform = train.transform)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:myenv]",
   "language": "python",
   "name": "conda-env-myenv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
